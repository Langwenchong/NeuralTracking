{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results of Neural Non-rigid traking using jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "import ipyvolume\n",
    "import json\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "\n",
    "# Modules \n",
    "from utils import image_proc\n",
    "from model.model import DeformNet\n",
    "from model import dataset\n",
    "\n",
    "import utils.utils as utils\n",
    "import utils.viz_utils as viz_utils\n",
    "import utils.nnutils as nnutils\n",
    "import utils.line_mesh as line_mesh_utils\n",
    "import options as opt\n",
    "\n",
    "\n",
    "# We will overwrite the default value in options.py / settings.py\n",
    "opt.use_mask = True\n",
    "o3d.visualization.RenderOption.line_width=160.0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSelector():\n",
    "    def __init__(self,jsonfile=\"../DeepDeform/test_graphs.json\"):\n",
    "        with open(jsonfile,'r') as f:\n",
    "            self.examples = json.load(f)\n",
    "        self.example = None\n",
    "        self.exampledropdown = ipywidgets.Dropdown(\\\n",
    "            options=[f\"{x['seq_id']}-{x['object_id']}-{x['source_id']}-{x['target_id']}\"\\\n",
    "                     for x in self.examples],\\\n",
    "            description=\"Example:\")\n",
    "\n",
    "        self.exampledropdown.observe(self.on_change)\n",
    "        \n",
    "        display(self.exampledropdown)\n",
    "        \n",
    "        self.split = jsonfile.split('/')[-1].split('_')[0]\n",
    "        \n",
    "        #####################################################################################################\n",
    "        # Load model\n",
    "        #####################################################################################################\n",
    "\n",
    "        saved_model = opt.saved_model\n",
    "\n",
    "        assert os.path.isfile(saved_model), f\"Model {saved_model} does not exist.\"\n",
    "        pretrained_dict = torch.load(saved_model)\n",
    "\n",
    "        # Construct model\n",
    "        self.model = DeformNet().cuda()\n",
    "\n",
    "        if \"chairs_things\" in saved_model:\n",
    "            self.model.flow_net.load_state_dict(pretrained_dict)\n",
    "        else:\n",
    "            if opt.model_module_to_load == \"full_model\":\n",
    "                # Load completely model            \n",
    "                self.model.load_state_dict(pretrained_dict)\n",
    "            elif opt.model_module_to_load == \"only_flow_net\":\n",
    "                # Load only optical flow part\n",
    "                self.model_dict = model.state_dict()\n",
    "                # 1. filter out unnecessary keys\n",
    "                pretrained_dict = {k: v for k, v in pretrained_dict.items() if \"flow_net\" in k}\n",
    "                # 2. overwrite entries in the existing state dict\n",
    "                self.model_dict.update(pretrained_dict) \n",
    "                # 3. load the new state dict\n",
    "                self.model.load_state_dict(model_dict)\n",
    "            else:\n",
    "                print(opt.model_module_to_load, \"is not a valid argument (A: 'full_model', B: 'only_flow_net')\")\n",
    "                exit()\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        \n",
    "    def on_change(self,change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            self.image_list = []\n",
    "            self.example = self.examples[self.exampledropdown.index]\n",
    "            for i in range(1,300,5):\n",
    "                if i%10 == 1: print(f\"{i}/100 Completed\")\n",
    "                source_ind = int(self.example['source_color'].split('/')[-1].split('.')[0])\n",
    "                self.example['target_color'] = \"/\".join(self.example['target_color'].split('/')[:-1]) + '/' + str(source_ind + i).zfill(6) + '.jpg'\n",
    "                \n",
    "                self.example['target_depth'] = \"/\".join(self.example['target_depth'].split('/')[:-1]) + '/' + str(source_ind + i).zfill(6) + '.png'\n",
    "                print(self.example['target_color'],self.example['target_depth'])\n",
    "                try: \n",
    "                    self.load_example()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\",e)\n",
    "                    continue\n",
    "            imageio.mimsave('movie.gif', self.image_list)\n",
    "    def load_example(self):\n",
    "        \n",
    "        global image_list\n",
    "        split = \"test\"\n",
    "        seq_id = self.example['seq_id']\n",
    "        src_id = self.example['source_id'] # source frame\n",
    "        tgt_id = self.example['target_id'] # target frame\n",
    "        \n",
    "        src_color_image_path         = os.path.join(\"../DeepDeform/\" , self.example['source_color'])\n",
    "        src_depth_image_path         = os.path.join(\"../DeepDeform/\" , self.example['source_depth'])\n",
    "        tgt_color_image_path         = os.path.join(\"../DeepDeform/\" , self.example['target_color'])\n",
    "        tgt_depth_image_path         = os.path.join(\"../DeepDeform/\" , self.example['target_depth'])\n",
    "        graph_nodes_path             = os.path.join(\"../DeepDeform/\" , self.example[\"graph_nodes\"])\n",
    "        graph_edges_path             = os.path.join(\"../DeepDeform/\" , self.example[\"graph_edges\"])\n",
    "        graph_edges_weights_path     = os.path.join(\"../DeepDeform/\" , self.example[\"graph_edges_weights\"])\n",
    "        graph_clusters_path          = os.path.join(\"../DeepDeform/\" , self.example[\"graph_clusters\"])\n",
    "        pixel_anchors_path           = os.path.join(\"../DeepDeform/\" , self.example[\"pixel_anchors\"])\n",
    "        pixel_weights_path           = os.path.join(\"../DeepDeform/\" , self.example[\"pixel_weights\"])\n",
    "\n",
    "        # Make sure to use the intrinsics corresponding to the seq_id above!!  \n",
    "        intrinsics = {\n",
    "            \"fx\": 575.548,\n",
    "            \"fy\": 577.46,\n",
    "            \"cx\": 323.172,\n",
    "            \"cy\": 236.417\n",
    "        }\n",
    "\n",
    "        # Some params for coloring the predicted correspondence confidences\n",
    "        weight_thr = 0.3\n",
    "        weight_scale = 1\n",
    "\n",
    "\n",
    "        image_height = opt.image_height\n",
    "        image_width  = opt.image_width\n",
    "        max_boundary_dist = opt.max_boundary_dist\n",
    "\n",
    "        \n",
    "        \n",
    "        # Source color and depth\n",
    "        source, _, cropper = dataset.DeformDataset.load_image(\n",
    "            src_color_image_path, src_depth_image_path, intrinsics, image_height, image_width\n",
    "        )\n",
    "\n",
    "        # Target color and depth (and boundary mask)\n",
    "        target, target_boundary_mask, _ = dataset.DeformDataset.load_image(\n",
    "            tgt_color_image_path, tgt_depth_image_path, intrinsics, image_height, image_width, cropper=cropper,\n",
    "            max_boundary_dist=max_boundary_dist, compute_boundary_mask=True\n",
    "        )\n",
    "\n",
    "        # Graph\n",
    "        graph_nodes, graph_edges, graph_edges_weights, _, graph_clusters, pixel_anchors, pixel_weights = dataset.DeformDataset.load_graph_data(\n",
    "            graph_nodes_path, graph_edges_path, graph_edges_weights_path, None, \n",
    "            graph_clusters_path, pixel_anchors_path, pixel_weights_path, cropper\n",
    "        )\n",
    "        \n",
    "        print(\"Graph Nodes\",graph_nodes.shape)\n",
    "        print(graph_nodes)\n",
    "        print(\"Graph Edges\",graph_edges.shape)\n",
    "        print(graph_edges)\n",
    "        print(\"Graph Edge Weights\",graph_edges_weights.shape)\n",
    "        print(graph_edges_weights)\n",
    "        print(graph_edges_weights[0,0], np.linalg.norm(graph_nodes[0] - graph_nodes[graph_edges[0,0]],ord=1))\n",
    "        print(\"Graph Clusters\",graph_clusters.shape)\n",
    "        print(graph_clusters)\n",
    "        print(\"Pixel Anchors\",pixel_anchors.shape)\n",
    "        print(pixel_anchors)\n",
    "        print(\"Pixel Anchors\",pixel_weights.shape)\n",
    "        print(pixel_weights)\n",
    "        \n",
    "        break\n",
    "\n",
    "        \n",
    "        num_nodes = np.array(graph_nodes.shape[0], dtype=np.int64)\n",
    "                \n",
    "        # Update intrinsics to reflect the crops\n",
    "        fx, fy, cx, cy = image_proc.modify_intrinsics_due_to_cropping(\n",
    "            intrinsics['fx'], intrinsics['fy'], intrinsics['cx'], intrinsics['cy'], \n",
    "            image_height, image_width, original_h=cropper.h, original_w=cropper.w\n",
    "        )\n",
    "\n",
    "        intrinsics = np.zeros((4), dtype=np.float32)\n",
    "        intrinsics[0] = fx\n",
    "        intrinsics[1] = fy\n",
    "        intrinsics[2] = cx\n",
    "        intrinsics[3] = cy\n",
    "        \n",
    "\n",
    "        #####################################################################################################\n",
    "        # Predict deformation\n",
    "        #####################################################################################################\n",
    "\n",
    "        # Move to device and unsqueeze in the batch dimension (to have batch size 1)\n",
    "        source_cuda               = torch.from_numpy(source).cuda().unsqueeze(0)\n",
    "        target_cuda               = torch.from_numpy(target).cuda().unsqueeze(0)\n",
    "        target_boundary_mask_cuda = torch.from_numpy(target_boundary_mask).cuda().unsqueeze(0)\n",
    "        graph_nodes_cuda          = torch.from_numpy(graph_nodes).cuda().unsqueeze(0)\n",
    "        graph_edges_cuda          = torch.from_numpy(graph_edges).cuda().unsqueeze(0)\n",
    "        graph_edges_weights_cuda  = torch.from_numpy(graph_edges_weights).cuda().unsqueeze(0)\n",
    "        graph_clusters_cuda       = torch.from_numpy(graph_clusters).cuda().unsqueeze(0)\n",
    "        pixel_anchors_cuda        = torch.from_numpy(pixel_anchors).cuda().unsqueeze(0)\n",
    "        pixel_weights_cuda        = torch.from_numpy(pixel_weights).cuda().unsqueeze(0)\n",
    "        intrinsics_cuda           = torch.from_numpy(intrinsics).cuda().unsqueeze(0)\n",
    "\n",
    "        num_nodes_cuda            = torch.from_numpy(num_nodes).cuda().unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_data = self.model(\n",
    "                source_cuda, target_cuda, \n",
    "                graph_nodes_cuda, graph_edges_cuda, graph_edges_weights_cuda, graph_clusters_cuda, \n",
    "                pixel_anchors_cuda, pixel_weights_cuda, \n",
    "                num_nodes_cuda, intrinsics_cuda, \n",
    "                evaluate=True, split=\"test\"\n",
    "            )\n",
    "\n",
    "        # Get some of the results\n",
    "        rotations_pred    = model_data[\"node_rotations\"].view(num_nodes, 3, 3).cpu().numpy()\n",
    "        translations_pred = model_data[\"node_translations\"].view(num_nodes, 3).cpu().numpy()\n",
    "\n",
    "        mask_pred = model_data[\"mask_pred\"]\n",
    "        assert mask_pred is not None, \"Make sure use_mask=True in options.py\"\n",
    "        mask_pred = mask_pred.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "\n",
    "        # Compute mask gt for mask baseline\n",
    "        _, source_points, valid_source_points, target_matches, \\\n",
    "            valid_target_matches, valid_correspondences, _, \\\n",
    "                _ = model_data[\"correspondence_info\"]\n",
    "        print(valid_correspondences.shape)\n",
    "        print(valid_correspondences)\n",
    "        target_matches        = target_matches.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "        valid_source_points   = valid_source_points.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "        valid_target_matches  = valid_target_matches.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "        valid_correspondences = valid_correspondences.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "\n",
    "        deformed_graph_nodes = graph_nodes + translations_pred        \n",
    "        \n",
    "        del source_cuda\n",
    "        del target_cuda\n",
    "        del target_boundary_mask_cuda\n",
    "        del graph_nodes_cuda\n",
    "        del graph_edges_cuda\n",
    "        del graph_edges_weights_cuda\n",
    "        del graph_clusters_cuda\n",
    "        del pixel_anchors_cuda\n",
    "        del pixel_weights_cuda\n",
    "        del intrinsics_cuda\n",
    "        \n",
    "        source_flat = np.moveaxis(source, 0, -1).reshape(-1, 6)\n",
    "        source_points = viz_utils.transform_pointcloud_to_opengl_coords(source_flat[..., 3:])\n",
    "        source_colors = source_flat[..., :3]\n",
    "\n",
    "        source_pcd = o3d.geometry.PointCloud()\n",
    "        source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "        source_pcd.colors = o3d.utility.Vector3dVector(source_colors)\n",
    "\n",
    "        # keep only object using the mask\n",
    "        valid_source_mask = np.moveaxis(valid_source_points, 0, -1).reshape(-1).astype(np.bool)\n",
    "        valid_source_points = source_points[valid_source_mask, :]\n",
    "        valid_source_colors = source_colors[valid_source_mask, :]\n",
    "        # source object PointCloud\n",
    "        source_object_pcd = o3d.geometry.PointCloud()\n",
    "        source_object_pcd.points = o3d.utility.Vector3dVector(valid_source_points)\n",
    "        source_object_pcd.colors = o3d.utility.Vector3dVector(valid_source_colors)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([source_pcd])\n",
    "        # o3d.visualization.draw_geometries([source_object_pcd])\n",
    "\n",
    "        #####################################################################################################\n",
    "        # Source warped\n",
    "        #####################################################################################################\n",
    "        warped_deform_pred_3d_np = image_proc.warp_deform_3d(\n",
    "            source, pixel_anchors, pixel_weights, graph_nodes, rotations_pred, translations_pred\n",
    "        )\n",
    "\n",
    "        source_warped = np.copy(source)\n",
    "        source_warped[3:, :, :] = warped_deform_pred_3d_np\n",
    "\n",
    "        # (source) warped RGB-D image\n",
    "        source_warped = np.moveaxis(source_warped, 0, -1).reshape(-1, 6)\n",
    "        warped_points = viz_utils.transform_pointcloud_to_opengl_coords(source_warped[..., 3:])\n",
    "        warped_colors = source_warped[..., :3]\n",
    "        # Filter points at (0, 0, 0)\n",
    "        warped_points = warped_points[valid_source_mask]\n",
    "        warped_colors = warped_colors[valid_source_mask]\n",
    "        # warped PointCloud\n",
    "        warped_pcd = o3d.geometry.PointCloud()\n",
    "        warped_pcd.points = o3d.utility.Vector3dVector(warped_points)\n",
    "        warped_pcd.paint_uniform_color([1, 0.706, 0]) # warped_pcd.colors = o3d.utility.Vector3dVector(warped_colors)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([source_object_pcd, warped_pcd])\n",
    "\n",
    "        ####################################\n",
    "        # TARGET #\n",
    "        ####################################\n",
    "        # target RGB-D image\n",
    "        target_flat = np.moveaxis(target, 0, -1).reshape(-1, 6)\n",
    "        target_points = viz_utils.transform_pointcloud_to_opengl_coords(target_flat[..., 3:])\n",
    "        target_colors = target_flat[..., :3]\n",
    "        # target PointCloud\n",
    "        target_pcd = o3d.geometry.PointCloud()\n",
    "        target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "        target_pcd.colors = o3d.utility.Vector3dVector(target_colors)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([target_pcd])\n",
    "\n",
    "        ####################################\n",
    "        # GRAPH #\n",
    "        ####################################\n",
    "\n",
    "        # Transform to OpenGL coords\n",
    "        graph_nodes = viz_utils.transform_pointcloud_to_opengl_coords(graph_nodes)\n",
    "        deformed_graph_nodes = viz_utils.transform_pointcloud_to_opengl_coords(deformed_graph_nodes)\n",
    "\n",
    "        # Graph nodes\n",
    "        rendered_graph_nodes = []\n",
    "        for node in graph_nodes:\n",
    "            mesh_sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.01)\n",
    "            mesh_sphere.compute_vertex_normals()\n",
    "            mesh_sphere.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "            mesh_sphere.translate(node)\n",
    "            rendered_graph_nodes.append(mesh_sphere)\n",
    "\n",
    "        # Merge all different sphere meshes\n",
    "        rendered_graph_nodes = viz_utils.merge_meshes(rendered_graph_nodes)\n",
    "\n",
    "        # Graph edges\n",
    "        edges_pairs = []\n",
    "        for node_id, edges in enumerate(graph_edges):\n",
    "            for neighbor_id in edges:\n",
    "                if neighbor_id == -1:\n",
    "                    break\n",
    "                edges_pairs.append([node_id, neighbor_id])    \n",
    "\n",
    "        colors = [[0.2, 1.0, 0.2] for i in range(len(edges_pairs))]\n",
    "        line_mesh = line_mesh_utils.LineMesh(graph_nodes, edges_pairs, colors, radius=0.003)\n",
    "        line_mesh_geoms = line_mesh.cylinder_segments\n",
    "\n",
    "        # Merge all different line meshes\n",
    "        line_mesh_geoms = viz_utils.merge_meshes(line_mesh_geoms)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([rendered_graph_nodes, line_mesh_geoms, source_object_pcd])\n",
    "\n",
    "        # Combined nodes & edges\n",
    "        rendered_graph = [rendered_graph_nodes, line_mesh_geoms]\n",
    "\n",
    "        ####################################\n",
    "        # Mask\n",
    "        ####################################\n",
    "        mask_pred_flat = mask_pred.reshape(-1)\n",
    "        valid_correspondences = valid_correspondences.reshape(-1).astype(np.bool)\n",
    "\n",
    "        ####################################\n",
    "        # Correspondences\n",
    "        ####################################\n",
    "        # target matches\n",
    "        target_matches = np.moveaxis(target_matches, 0, -1).reshape(-1, 3)\n",
    "        target_matches = viz_utils.transform_pointcloud_to_opengl_coords(target_matches)\n",
    "\n",
    "        ################################\n",
    "        # \"Good\" matches\n",
    "        ################################\n",
    "        good_mask = valid_correspondences & (mask_pred_flat >= weight_thr)\n",
    "        good_source_points_corresp  = source_points[good_mask]\n",
    "        good_target_matches_corresp = target_matches[good_mask]\n",
    "        good_mask_pred              = mask_pred_flat[good_mask]\n",
    "\n",
    "        # number of good matches\n",
    "        n_good_matches = good_source_points_corresp.shape[0]\n",
    "        # Subsample\n",
    "        subsample = True\n",
    "        if subsample:\n",
    "            N = 2000\n",
    "            sampled_idxs = np.random.permutation(n_good_matches)[:N]\n",
    "            good_source_points_corresp  = good_source_points_corresp[sampled_idxs]\n",
    "            good_target_matches_corresp = good_target_matches_corresp[sampled_idxs]\n",
    "            good_mask_pred              = good_mask_pred[sampled_idxs]\n",
    "            n_good_matches = N\n",
    "        # both good_source and good_target points together into one vector\n",
    "        good_matches_points = np.concatenate([good_source_points_corresp, good_target_matches_corresp], axis=0)\n",
    "        good_matches_lines = [[i, i + n_good_matches] for i in range(0, n_good_matches, 1)]\n",
    "\n",
    "        # --> Create good (unweighted) lines \n",
    "        good_matches_colors = [[201/255, 177/255, 14/255] for i in range(len(good_matches_lines))]\n",
    "        good_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(good_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(good_matches_lines),\n",
    "        )\n",
    "        good_matches_set.colors = o3d.utility.Vector3dVector(good_matches_colors)\n",
    "\n",
    "        # --> Create good weighted lines \n",
    "        # first, we need to get the proper color coding\n",
    "        high_color, low_color = np.array([0.0, 0.8, 0]), np.array([0.8, 0, 0.0])\n",
    "\n",
    "        good_weighted_matches_colors = np.ones_like(good_source_points_corresp)\n",
    "\n",
    "        weights_normalized = np.maximum(np.minimum(0.5 + (good_mask_pred - weight_thr) / weight_scale, 1.0), 0.0)\n",
    "        weights_normalized_opposite = 1 - weights_normalized\n",
    "\n",
    "        good_weighted_matches_colors[:, 0] = weights_normalized * high_color[0] + weights_normalized_opposite * low_color[0]\n",
    "        good_weighted_matches_colors[:, 1] = weights_normalized * high_color[1] + weights_normalized_opposite * low_color[1]\n",
    "        good_weighted_matches_colors[:, 2] = weights_normalized * high_color[2] + weights_normalized_opposite * low_color[2]\n",
    "\n",
    "        good_weighted_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(good_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(good_matches_lines),\n",
    "        )\n",
    "        good_weighted_matches_set.colors = o3d.utility.Vector3dVector(good_weighted_matches_colors)\n",
    "\n",
    "\n",
    "        ################################\n",
    "        # \"Bad\" matches\n",
    "        ################################\n",
    "        bad_mask = valid_correspondences & (mask_pred_flat < weight_thr)\n",
    "        bad_source_points_corresp  = source_points[bad_mask]\n",
    "        bad_target_matches_corresp = target_matches[bad_mask]\n",
    "        bad_mask_pred              = mask_pred_flat[bad_mask]\n",
    "\n",
    "        # number of good matches\n",
    "        n_bad_matches = bad_source_points_corresp.shape[0]\n",
    "\n",
    "        # both good_source and good_target points together into one vector\n",
    "        bad_matches_points = np.concatenate([bad_source_points_corresp, bad_target_matches_corresp], axis=0)\n",
    "        bad_matches_lines = [[i, i + n_bad_matches] for i in range(0, n_bad_matches, 1)]\n",
    "\n",
    "        # --> Create bad (unweighted) lines \n",
    "        bad_matches_colors = [[201/255, 177/255, 14/255] for i in range(len(bad_matches_lines))]\n",
    "        bad_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(bad_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(bad_matches_lines),\n",
    "        )\n",
    "        bad_matches_set.colors = o3d.utility.Vector3dVector(bad_matches_colors)\n",
    "\n",
    "        # --> Create bad weighted lines \n",
    "        # first, we need to get the proper color coding\n",
    "        high_color, low_color = np.array([0.0, 0.8, 0]), np.array([0.8, 0, 0.0])\n",
    "\n",
    "        bad_weighted_matches_colors = np.ones_like(bad_source_points_corresp)\n",
    "\n",
    "        weights_normalized = np.maximum(np.minimum(0.5 + (bad_mask_pred - weight_thr) / weight_scale, 1.0), 0.0)\n",
    "        weights_normalized_opposite = 1 - weights_normalized\n",
    "\n",
    "        bad_weighted_matches_colors[:, 0] = weights_normalized * high_color[0] + weights_normalized_opposite * low_color[0]\n",
    "        bad_weighted_matches_colors[:, 1] = weights_normalized * high_color[1] + weights_normalized_opposite * low_color[1]\n",
    "        bad_weighted_matches_colors[:, 2] = weights_normalized * high_color[2] + weights_normalized_opposite * low_color[2]\n",
    "\n",
    "        bad_weighted_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(bad_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(bad_matches_lines),\n",
    "        )\n",
    "        bad_weighted_matches_set.colors = o3d.utility.Vector3dVector(bad_weighted_matches_colors)\n",
    "\n",
    "        ####################################\n",
    "        # Generate info for aligning source to target (by interpolating between source and warped source)\n",
    "        ####################################\n",
    "        assert warped_points.shape[0] == valid_source_points.shape[0]\n",
    "        line_segments = warped_points - valid_source_points\n",
    "        line_segments_unit, line_lengths = line_mesh_utils.normalized(line_segments)\n",
    "        line_lengths = line_lengths[:, np.newaxis]\n",
    "        line_lengths = np.repeat(line_lengths, 3, axis=1)\n",
    "\n",
    "        ####################################\n",
    "        # Draw \n",
    "        ####################################\n",
    "        geometry_dict = {\n",
    "            \"source_pcd\": source_pcd, \n",
    "            \"source_obj\": source_object_pcd, \n",
    "            \"target_pcd\": target_pcd, \n",
    "            \"graph\":      rendered_graph,\n",
    "            # \"deformed_graph\":    rendered_deformed_graph\n",
    "        }\n",
    "\n",
    "        alignment_dict = {\n",
    "            \"valid_source_points\": valid_source_points,\n",
    "            \"line_segments_unit\":  line_segments_unit,\n",
    "            \"line_lengths\":        line_lengths\n",
    "        }\n",
    "\n",
    "        matches_dict = {\n",
    "            \"good_matches_set\":          good_matches_set,\n",
    "            \"good_weighted_matches_set\": good_weighted_matches_set,\n",
    "            \"bad_matches_set\":           bad_matches_set,\n",
    "            \"bad_weighted_matches_set\":  bad_weighted_matches_set\n",
    "        }\n",
    "\n",
    "        # Save image \n",
    "        sourceimage = create_image([source_object_pcd,good_weighted_matches_set])\n",
    "        warpedimage = create_image([warped_pcd])\n",
    "        targetimage = create_image([target_pcd],w=1280,h=960)\n",
    "        image = cv2.hconcat([cv2.vconcat([sourceimage,warpedimage]),targetimage])\n",
    "        \n",
    "        self.image_list.append(image)\n",
    "        #####################################################################################################\n",
    "        # Open viewer\n",
    "        #####################################################################################################\n",
    "#         manager = viz_utils.CustomDrawGeometryWithKeyCallback(\n",
    "#             geometry_dict, alignment_dict, matches_dict\n",
    "#         )\n",
    "#         manager.custom_draw_geometry_with_key_callback()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(point_cloud_list,w=640,h=480,sideview=False):\n",
    "\tvis = o3d.visualization.Visualizer()\n",
    "\tvis.create_window(width=w, height=h)\n",
    "\t# Read camera params\n",
    "\t# param = o3d.io.read_pinhole_camera_parameters('cameraparams.json')\n",
    "\tparam = o3d.io.read_pinhole_camera_parameters('./viewpoint.json' if w == 640 else './viewpoint_big.json')\n",
    "\tctr = vis.get_view_control()\n",
    "\tctr.convert_from_pinhole_camera_parameters(param)\n",
    "\tfor pc in point_cloud_list:\n",
    "\t\tvis.add_geometry(pc)\n",
    "\n",
    "\n",
    "\tif sideview: \n",
    "\t\tctr.rotate(480,0) # Front view\n",
    "\telse:\n",
    "\t\tctr.rotate(0, 0) # Side view\n",
    "\n",
    "\tvis.poll_events()\n",
    "\tvis.update_renderer()\n",
    "\timage = vis.capture_screen_float_buffer()\n",
    "\timage = np.asarray(image) * 255\n",
    "\timage = image.astype(np.uint8)\n",
    "\n",
    "# \tvis.run()\n",
    "\t# Close\n",
    "\tvis.destroy_window()\n",
    "\treturn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218adfa942d14cf0840b75cf54785d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Example:', options=('seq000-blackdog-000000-000200', 'seq000-blackdog-000000-001200', 'sâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 Completed\n",
      "train/seq028/color/000051.jpg train/seq028/depth/000051.png\n",
      "Graph Nodes (208, 3)\n",
      "[[-0.20947239  0.2787412   0.704     ]\n",
      " [-0.09223369  0.2791858   0.696     ]\n",
      " [ 0.0336504   0.27676147  0.699     ]\n",
      " [-0.3285019  -0.40425992  1.21      ]\n",
      " [-0.24472183 -0.39865717  1.181     ]\n",
      " [-0.28884578  0.27758133  0.692     ]\n",
      " [-0.14670879  0.2693017   0.708     ]\n",
      " [-0.33590087  0.26173824  0.685     ]\n",
      " [-0.38577354 -0.405033    1.225     ]\n",
      " [-0.16728395 -0.41197646  1.246     ]\n",
      " [-0.01864186  0.26459068  0.702     ]\n",
      " [-0.25210458  0.25803173  0.721     ]\n",
      " [-0.3054478  -0.37875167  1.17      ]\n",
      " [-0.11109734 -0.4036781   1.247     ]\n",
      " [-0.56634045 -0.4002329   1.243     ]\n",
      " [-0.49292046 -0.41665438  1.294     ]\n",
      " [-0.4511888  -0.3950491   1.247     ]\n",
      " [-0.26311913 -0.3686779   1.145     ]\n",
      " [ 0.00988799  0.24113506  0.738     ]\n",
      " [-0.35628283  0.2340557   0.724     ]\n",
      " [-0.17842938  0.22747952  0.727     ]\n",
      " [-0.04016414  0.23762646  0.739     ]\n",
      " [-0.30301753  0.23179272  0.717     ]\n",
      " [-0.1841915  -0.37632096  1.201     ]\n",
      " [-0.1334944   0.2408469   0.774     ]\n",
      " [-0.23050229  0.22639003  0.757     ]\n",
      " [-0.18195257  0.22787097  0.78      ]\n",
      " [-0.06127501 -0.3950491   1.247     ]\n",
      " [-0.06601381  0.21630132  0.787     ]\n",
      " [-0.27967745  0.21206665  0.762     ]\n",
      " [-0.32912773  0.1986118   0.751     ]\n",
      " [-0.37030566 -0.370504    1.189     ]\n",
      " [-0.1141521   0.19364715  0.789     ]\n",
      " [-0.597171   -0.369236    1.205     ]\n",
      " [-0.23166601  0.1954504   0.802     ]\n",
      " [-0.38000622  0.18302146  0.751     ]\n",
      " [-0.01724598  0.19132143  0.808     ]\n",
      " [-0.16121519  0.18225539  0.805     ]\n",
      " [-0.20687339 -0.3572914   1.153     ]\n",
      " [-0.30838412  0.16886416  0.788     ]\n",
      " [-0.52060336 -0.38700834  1.263     ]\n",
      " [-0.06752363  0.1669367   0.805     ]\n",
      " [-0.26775473  0.16497448  0.823     ]\n",
      " [-0.19768357  0.16160187  0.835     ]\n",
      " [-0.36085778  0.14587462  0.789     ]\n",
      " [ 0.01698965  0.14860135  0.835     ]\n",
      " [-0.4195766  -0.3692842   1.212     ]\n",
      " [-0.29108912 -0.3397293   1.115     ]\n",
      " [-0.13534915 -0.3734776   1.212     ]\n",
      " [-0.12276059  0.14433706  0.819     ]\n",
      " [-0.34188035 -0.34810108  1.149     ]\n",
      " [-0.23973484  0.14578101  0.861     ]\n",
      " [-0.03337318  0.13397682  0.825     ]\n",
      " [-0.31790042  0.13430163  0.827     ]\n",
      " [-0.15767676  0.12834468  0.854     ]\n",
      " [-0.28559628  0.12237418  0.864     ]\n",
      " [-0.08193734  0.11724189  0.838     ]\n",
      " [-0.38732487  0.11584282  0.828     ]\n",
      " [-0.20563349  0.11030883  0.875     ]\n",
      " [-0.07614613 -0.36388594  1.208     ]\n",
      " [-0.33516872  0.10942635  0.868     ]\n",
      " [-0.54635036 -0.36329365  1.213     ]\n",
      " [-0.25299785  0.10759014  0.903     ]\n",
      " [-0.12133726  0.09763804  0.87      ]\n",
      " [-0.23791838 -0.33156505  1.12      ]\n",
      " [-0.385871    0.08476913  0.877     ]\n",
      " [-0.29962087  0.08204702  0.897     ]\n",
      " [-0.49211064  0.08515695  0.931     ]\n",
      " [-0.02417529 -0.36288455  1.233     ]\n",
      " [-0.22138003  0.06846064  0.902     ]\n",
      " [ 0.29570752  0.07879311  1.015     ]\n",
      " [-0.15968546  0.07216883  0.89      ]\n",
      " [ 0.34796602  0.0795967   1.003     ]\n",
      " [-0.35090363  0.06959915  0.917     ]\n",
      " [-0.40965703  0.0718002   0.946     ]\n",
      " [-0.2697956   0.06119478  0.934     ]\n",
      " [-0.5742188  -0.33450848  1.171     ]\n",
      " [ 0.3959602   0.06038955  1.001     ]\n",
      " [ 0.20635563  0.05719286  0.976     ]\n",
      " [-0.16478994 -0.32733777  1.139     ]\n",
      " [-0.49591404 -0.34895065  1.229     ]\n",
      " [-0.38780358 -0.32964343  1.161     ]\n",
      " [-0.32065827  0.04653186  0.965     ]\n",
      " [ 0.32679075  0.04839566  1.041     ]\n",
      " [-0.16938367  0.04670277  0.935     ]\n",
      " [-0.44951403  0.04207405  0.94      ]\n",
      " [ 0.13542455  0.04377493  0.978     ]\n",
      " [ 0.08393943  0.03846186  0.972     ]\n",
      " [ 0.37596214  0.03599581  1.047     ]\n",
      " [-0.11927426 -0.33854598  1.178     ]\n",
      " [ 0.26390088  0.03988636  1.008     ]\n",
      " [-0.37352473  0.03643978  0.963     ]\n",
      " [ 0.44991666  0.03281328  1.005     ]\n",
      " [-0.21784884  0.03095219  0.948     ]\n",
      " [-0.49105346  0.01586774  0.929     ]\n",
      " [-0.15106457  0.01852825  0.985     ]\n",
      " [-0.26246879  0.021669    0.973     ]\n",
      " [ 0.1746006   0.01359335  0.998     ]\n",
      " [ 0.29747087  0.00158042  1.046     ]\n",
      " [-0.3395723   0.00147465  0.976     ]\n",
      " [ 0.42379266  0.00156382  1.035     ]\n",
      " [ 0.03140146  0.0031307   0.966     ]\n",
      " [-0.41216996  0.00143236  0.948     ]\n",
      " [-0.20623651  0.00149131  0.987     ]\n",
      " [ 0.24009687 -0.00199962  1.026     ]\n",
      " [ 0.11278147 -0.00705314  0.988     ]\n",
      " [ 0.36191654 -0.01504272  1.07      ]\n",
      " [-0.44631827 -0.3259133   1.199     ]\n",
      " [-0.02735883 -0.00690323  0.967     ]\n",
      " [-0.35207742 -0.30969805  1.118     ]\n",
      " [-0.0833914  -0.01369312  0.974     ]\n",
      " [-0.05783534 -0.32807785  1.177     ]\n",
      " [-0.29018354 -0.0235585   0.964     ]\n",
      " [-0.16138682 -0.03226084  0.975     ]\n",
      " [-0.46454558 -0.02470285  0.944     ]\n",
      " [ 0.26370156 -0.0419682   1.049     ]\n",
      " [ 0.19732207 -0.0321734   1.026     ]\n",
      " [-0.36682853 -0.03513137  1.009     ]\n",
      " [ 0.06753749 -0.03069955  0.979     ]\n",
      " [-0.23733312 -0.03701472  0.967     ]\n",
      " [-0.30000252 -0.29276097  1.098     ]\n",
      " [ 0.31819683 -0.04478464  1.073     ]\n",
      " [ 0.14672822 -0.04211342  1.009     ]\n",
      " [ 0.00798231 -0.04575443  0.975     ]\n",
      " [-0.39855546 -0.04388696  0.971     ]\n",
      " [-0.5194535  -0.3275842   1.19      ]\n",
      " [-0.1146647  -0.05452138  0.981     ]\n",
      " [-0.32294858 -0.05936956  0.977     ]\n",
      " [-0.04147226 -0.05803362  0.983     ]\n",
      " [-0.50313765 -0.0613369   0.955     ]\n",
      " [ 0.0928393  -0.07251248  0.995     ]\n",
      " [ 0.22660685 -0.07622918  1.046     ]\n",
      " [-0.44332346 -0.06677911  0.962     ]\n",
      " [ 0.17627752 -0.07847419  1.028     ]\n",
      " [-0.19493425 -0.07199556  0.965     ]\n",
      " [-0.3843578  -0.07986224  1.023     ]\n",
      " [ 0.29643148 -0.0900836   1.082     ]\n",
      " [-0.26924923 -0.07716337  0.967     ]\n",
      " [ 0.03718996 -0.08891389  0.986     ]\n",
      " [-0.15075785 -0.09544551  0.983     ]\n",
      " [-0.6070771  -0.30817384  1.141     ]\n",
      " [-0.08074671 -0.09884658  0.983     ]\n",
      " [ 0.20251471 -0.1204586   1.053     ]\n",
      " [-0.47836787 -0.10013798  0.979     ]\n",
      " [ 0.13388985 -0.1076493   1.018     ]\n",
      " [-0.34653074 -0.10187683  0.996     ]\n",
      " [-0.01587451 -0.11086303  0.984     ]\n",
      " [-0.42732617 -0.10474405  1.007     ]\n",
      " [ 0.2508614  -0.11706859  1.072     ]\n",
      " [-0.01106691 -0.32337505  1.205     ]\n",
      " [ 0.08696187 -0.12390667  1.007     ]\n",
      " [-0.21367183 -0.11816123  0.974     ]\n",
      " [-0.41638982 -0.12919763  1.05      ]\n",
      " [-0.52673656 -0.11667699  0.99      ]\n",
      " [-0.30010694 -0.12397519  0.98      ]\n",
      " [ 0.03086185 -0.1372956   1.003     ]\n",
      " [-0.46774775 -0.13542646  1.015     ]\n",
      " [-0.36656374 -0.14287105  1.018     ]\n",
      " [-0.12086204 -0.14035347  0.976     ]\n",
      " [ 0.11031711 -0.15403882  1.046     ]\n",
      " [-0.05331662 -0.14446661  0.981     ]\n",
      " [ 0.16908252 -0.16358367  1.073     ]\n",
      " [-0.25584513 -0.15279612  0.98      ]\n",
      " [-0.51330864 -0.15626583  1.025     ]\n",
      " [-0.12150154 -0.28633028  1.088     ]\n",
      " [-0.4414194  -0.17025587  1.08      ]\n",
      " [-0.06656939 -0.28768265  1.086     ]\n",
      " [-0.17002946 -0.1538609   0.976     ]\n",
      " [ 0.06216991 -0.17144918  1.031     ]\n",
      " [-0.38936526 -0.17756268  1.046     ]\n",
      " [-0.3161768  -0.17043298  1.004     ]\n",
      " [ 0.19119596 -0.19120461  1.115     ]\n",
      " [-0.00223351 -0.17476887  0.999     ]\n",
      " [ 0.12562305 -0.18776387  1.084     ]\n",
      " [-0.4944898  -0.18701494  1.069     ]\n",
      " [-0.241234   -0.283345    1.091     ]\n",
      " [-0.09055629 -0.17786852  0.997     ]\n",
      " [-0.45582262 -0.19981214  1.12      ]\n",
      " [-0.21890163 -0.1903209   0.99      ]\n",
      " [-0.4152137  -0.21618931  1.095     ]\n",
      " [ 0.07850099 -0.21254483  1.058     ]\n",
      " [-0.13863131 -0.19796814  0.994     ]\n",
      " [-0.2739326  -0.20095557  1.009     ]\n",
      " [-0.34032264 -0.20255281  1.035     ]\n",
      " [-0.5096837  -0.22027436  1.106     ]\n",
      " [-0.5555438  -0.29238287  1.141     ]\n",
      " [ 0.02629936 -0.20849907  1.029     ]\n",
      " [-0.04612029 -0.20464922  1.01      ]\n",
      " [-0.44954848 -0.2428601   1.159     ]\n",
      " [-0.20149449 -0.29255086  1.134     ]\n",
      " [-0.11166239 -0.21624817  1.032     ]\n",
      " [ 0.12196612 -0.23472896  1.102     ]\n",
      " [-0.37238747 -0.2268479   1.065     ]\n",
      " [-0.43456385 -0.29374373  1.162     ]\n",
      " [-0.18333289 -0.22431225  1.012     ]\n",
      " [-0.56893986 -0.23767595  1.098     ]\n",
      " [-0.15251952 -0.23650315  1.067     ]\n",
      " [ 0.02946669 -0.31484306  1.237     ]\n",
      " [-0.3004148  -0.23231767  1.04      ]\n",
      " [-0.50224346 -0.26185358  1.137     ]\n",
      " [-0.23399082 -0.2345151   1.026     ]\n",
      " [-0.0149858  -0.23974456  1.041     ]\n",
      " [-0.32697797 -0.24872635  1.08      ]\n",
      " [-0.39233163 -0.25970966  1.111     ]\n",
      " [ 0.04602027 -0.25244743  1.072     ]\n",
      " [-0.0653041  -0.24396975  1.036     ]\n",
      " [-0.47230506 -0.2784106   1.191     ]\n",
      " [-0.20433798 -0.26720062  1.057     ]]\n",
      "Graph Edges (208, 8)\n",
      "[[ 11  20   6 ...  -1  -1  -1]\n",
      " [  6  21  10 ...  -1  -1  -1]\n",
      " [ 10  18  21 ...  -1  -1  -1]\n",
      " ...\n",
      " [201 187 190 ... 176 196  -1]\n",
      " [193 199 107 ...  80  -1  -1]\n",
      " [175 200 194 ...  -1  -1  -1]]\n",
      "Graph Edge Weights (208, 8)\n",
      "[[0.35495627 0.235319   0.2015655  ... 0.         0.         0.        ]\n",
      " [0.50776637 0.27891546 0.2133182  ... 0.         0.         0.        ]\n",
      " [0.44567087 0.42427382 0.1300553  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.23539153 0.2130498  0.20633586 ... 0.07137065 0.05487045 0.        ]\n",
      " [0.22583771 0.18836515 0.18268442 ... 0.07501396 0.         0.        ]\n",
      " [0.40037274 0.33742067 0.2622066  ... 0.         0.         0.        ]]\n",
      "0.35495627 0.0803417\n",
      "Graph Clusters (208, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "Pixel Anchors (448, 640, 4)\n",
      "[[[-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]]\n",
      "\n",
      " [[-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]]\n",
      "\n",
      " [[-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]]\n",
      "\n",
      " [[-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]]\n",
      "\n",
      " [[-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  ...\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]\n",
      "  [-1 -1 -1 -1]]]\n",
      "Pixel Anchors (448, 640, 4)\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 448, 640])\n",
      "tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "example_selector = ExampleSelector(jsonfile=\"../DeepDeform/train_graphs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_depth_image(depth_path,thresh=0):\n",
    "\tdepth_image = np.array(plt.imread(depth_path))\n",
    "\n",
    "\tf_x = 525\n",
    "\tf_y = 525\n",
    "\tc_x = 319.5\n",
    "\tc_y = 239.5\n",
    "\n",
    "\ty,x = np.where(depth_image > thresh)\n",
    "\tz = depth_image[y,x]\n",
    "\tpoint_cloud = np.array([ z*(x - c_x)/f_x,  z*(y - c_y)/f_y, z]).T\n",
    "\tpoint2frame = np.array([[y[i],x[i]] for i in range(point_cloud.shape[0]) ])    \n",
    "\treturn point_cloud,point2frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-046772715ca9>, line 108)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-046772715ca9>\"\u001b[0;36m, line \u001b[0;32m108\u001b[0m\n\u001b[0;31m    dmodel[]\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DeformableModel():\n",
    "\tdef __init__(self,sourcepath,grid_size,k=8,use3D=False):\n",
    "\t\tsuper(DeformableModel,self).__init__()\n",
    "\t\t\"\"\"\n",
    "\t\t\tHere we define our deformable model\n",
    "\t\t\t@param => datapath => Path to the source depth scan/mesh/point cloud :: file path::\n",
    "\t\t\t@param => grid_size => Voxel grid size to perform uniform sampling for creating deformable model  :: float \n",
    "\t\t\t@param => use3D => # Connectivity is better defined as the image instead of the dicontinouty present in depth scan :: bool\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.k = k # Copy data for later use\n",
    "\t\tself.use3D = use3D\n",
    "\n",
    "\t\tself.vertices,self.point2frame = read_depth_image(sourcepath)\n",
    "\t\tself.nodes = self.get_nodes_from_pointcloud(grid_size)\n",
    "\t\tself.N = self.nodes.shape[0]\t\n",
    "\t\tself.V = self.vertices.shape[0]\t\n",
    "\n",
    "\t\tself.create_adj_list(k=k) # Create edges and mapping for graph\n",
    "\t\tprint(f\"Deformable Model: Initialization Complete, Node:{self.nodes.shape[0]} Edges:{self.adj_matrix[0].shape[0]//2}\")\n",
    "\n",
    "\n",
    "\tdef get_nodes_from_pointcloud(self,grid_size):\n",
    "\t\t\"\"\"\n",
    "\t\t\tCreate the deformable model by uniform sampling and choosing k points \n",
    "\t\t\t# Refernce: https://github.com/pglira/Point_cloud_tools_for_Matlab/blob/master/classes/4pointCloud/uniformSampling.m ; \n",
    "\t\t\t@param => grid_size => Voxel grid size to perform uniform sampling for creating deformable model  :: float \n",
    "\t\t\treturn => node :: Graph nodes :: Nx3 np.array     \n",
    "\t\t\"\"\"\n",
    "\t\tvertices = self.vertices\n",
    "\t\tV = vertices.shape[0]\n",
    "\n",
    "\t\tminPoi = np.min(vertices,axis=0,keepdims=True) # Point with smallest coordinates\n",
    "\t\tlocalOrigin = np.floor(minPoi/100)*100 # Rounded local origin for voxel structure (voxels of different pcs have coincident voxel centers if mod(100, voxelSize) == 0)\n",
    "\t\t\n",
    "\t\t# Find 2/3-dimensional indices of voxels in which points are lying\n",
    "\t\tidxVoxelUnique = np.floor((vertices - localOrigin)/grid_size)\n",
    "\t\t# Remove multiple voxels\n",
    "\t\tidxVoxelUnique, indUnique = np.unique(idxVoxelUnique,axis=0,return_inverse=True)\t# indUnique contains \"voxel index\" for each point\t\n",
    "\n",
    "\t\tVoxelCenter = localOrigin + grid_size/2 + idxVoxelUnique*grid_size\n",
    "\n",
    "\t\tN = idxVoxelUnique.shape[0] # Number of unique points\n",
    "\t\t# Select points nearest to voxel centers ---------------------------------------\n",
    "\t\tidxSort = np.argsort(indUnique)\n",
    "\t\tindUnique = indUnique[idxSort]\n",
    "\t\tvertices = vertices[idxSort]\n",
    "\t\tidxJump = np.where(np.diff(indUnique))[0] # Sort indices and points (in order to find points inside of voxels very fast in the next loop)\n",
    "\t\tind_list = []\n",
    "\t\tfor i in range(N):\n",
    "    \t\t# Distance of points to voxel center\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tdist2voxelCenter = np.linalg.norm(vertices[:idxJump[0]+1] - VoxelCenter[i],axis=1)\n",
    "\t\t\t\tselected_ind = np.argmin(dist2voxelCenter)\n",
    "\t\t\telif i == N-1:\n",
    "\t\t\t\tdist2voxelCenter = np.linalg.norm(vertices[idxJump[i-1]+1:] - VoxelCenter[i],axis=1)\n",
    "\t\t\t\tselected_ind = np.argmin(dist2voxelCenter) + idxJump[i-1] +1\n",
    "\t\t\telse:\n",
    "\t\t\t\tdist2voxelCenter = np.linalg.norm(vertices[idxJump[i-1]+1:idxJump[i]+1] - VoxelCenter[i],axis=1)\n",
    "\t\t\t\tselected_ind = np.argmin(dist2voxelCenter) + idxJump[i-1] + 1 \n",
    "\n",
    "\t\t\tselected_ind = idxSort[selected_ind]\t\n",
    "\t\t\tind_list.append(selected_ind)\n",
    "\n",
    "\t\tself.node2vert = sorted(ind_list)\n",
    "\t\treturn self.vertices[self.node2vert]\n",
    "\n",
    "\tdef create_adj_list(self,k=4):\n",
    "\t\t\"\"\"\n",
    "\t\t\tDefine the neighbourhood, vertex-node mapping & corresponding weight-matrix as described in the paper \t\n",
    "\t\t\t@param => k:: int :: number of closest points on the graph for every vertex as defined in the paper\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tif self.use3D == True:\n",
    "\t\t\tkdtree = KDTree(self.nodes) # Use KD-tree for faster computation\n",
    "\t\t\t_,closest_neighbours = kdtree.query(self.vertices,k=k+1) # Find the k+1 clostest nodes for every vertex on the source points\n",
    "\t\telse:\t\n",
    "\t\t\tkdtree = KDTree(self.nodes[:,:2]) # Use KD-tree for faster computation\n",
    "\t\t\t_,closest_neighbours = kdtree.query(self.vertices[:,:2],k=k+1) # Find the k+1 clostest nodes for every vertex on the source points\n",
    "\n",
    "\t\tself.v2n = closest_neighbours[:,:-1] # Define the vertex to node mapping  (closest k)\n",
    "\n",
    "\t\tN = self.nodes.shape[0]\n",
    "\t\tself.adj_matrix = np.zeros((N,N),dtype=np.int32) # Define adj matrix\n",
    "\t\t\n",
    "\t\tst = time.time()\n",
    "\n",
    "\t\t# For every vertex connect all distinct k vertexes\n",
    "\t\tfor nn in np.unique(closest_neighbours,axis=1):\n",
    "\t\t\tinds = np.unique(nn[:-1])\n",
    "\t\t\tif len(inds) > 1:\n",
    "\t\t\t\tfor i,x in enumerate(inds):\n",
    "\t\t\t\t\tfor j,y in enumerate(inds[i+1:]):\n",
    "\t\t\t\t\t\tself.adj_matrix[x,y] = self.adj_matrix[y,x] = 1\n",
    "\n",
    "\t\t# Calculate the weight of influence from every node to vertex\n",
    "\t\tif self.use3D:\n",
    "\t\t\tself.w2n = np.linalg.norm(self.vertices[:,None,:] - self.nodes[closest_neighbours,:],axis=2)\n",
    "\t\telse:\n",
    "\t\t\tself.w2n = np.linalg.norm(self.vertices[:,None,:2] - self.nodes[closest_neighbours,:2],axis=2)\n",
    "\n",
    "\t\tself.w2n = 1 - self.w2n[:,:k]/self.w2n[:,k:k+1]\n",
    "\t\tself.w2n = self.w2n/np.sum(self.w2n,axis=1,keepdims=True)\n",
    "    \n",
    "\tdef generate(self):\n",
    "\t\tedges = np.where(self.adj_matrix==1)        \n",
    "\t\tdmodel[\"graph_nodes\"] = self.nodes\n",
    "\t\tN = self.nodes[0]        \n",
    "\t\tdmodel[\"graph_edges\"] = -1*np.ones((self.nodes,self.k))\n",
    "\t\tdmodel[\"graph_edge_weights\"] = np.zeros((self.nodes,self.k))\n",
    "\t\tfor i in range(N):\n",
    "\t\t\tcnt = 0     \n",
    "\t\t\tfor j in range(N):\n",
    "\t\t\t\tif self.adj_matrix[i,j] == 1:\n",
    "\t\t\t\tdmodel[\"graph_edges\"][i,cnt] = j\n",
    "\t\t\t\t\tcnt += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBDVideoResults():\n",
    "    def __init__(self,datapath=\"\",source_id=0,gridsize=0.003):\n",
    "\n",
    "        saved_model = opt.saved_model\n",
    "\n",
    "        assert os.path.isfile(saved_model), f\"Model {saved_model} does not exist.\"\n",
    "        pretrained_dict = torch.load(saved_model)\n",
    "\n",
    "        # Construct model\n",
    "        self.model = DeformNet().cuda()\n",
    "\n",
    "        if \"chairs_things\" in saved_model:\n",
    "            self.model.flow_net.load_state_dict(pretrained_dict)\n",
    "        else:\n",
    "            if opt.model_module_to_load == \"full_model\":\n",
    "                # Load completely model            \n",
    "                self.model.load_state_dict(pretrained_dict)\n",
    "            elif opt.model_module_to_load == \"only_flow_net\":\n",
    "                # Load only optical flow part\n",
    "                self.model_dict = model.state_dict()\n",
    "                # 1. filter out unnecessary keys\n",
    "                pretrained_dict = {k: v for k, v in pretrained_dict.items() if \"flow_net\" in k}\n",
    "                # 2. overwrite entries in the existing state dict\n",
    "                self.model_dict.update(pretrained_dict) \n",
    "                # 3. load the new state dict\n",
    "                self.model.load_state_dict(model_dict)\n",
    "            else:\n",
    "                print(opt.model_module_to_load, \"is not a valid argument (A: 'full_model', B: 'only_flow_net')\")\n",
    "                exit()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.datapath = datapath \n",
    "        \n",
    "        filelist = sorted(os.listdir(os.path.join(datapath,\"depth\")),key=lambda x: int(x.split('.')[0]))\n",
    "        sourcefile = filelist[source_id]\n",
    "        dmodel = DeformableModel(os.path.join(datapath,\"depth\",sourcefile),gridsize).generate()\n",
    "        for targetfile in filelist:\n",
    "            print(f\"{i}/{len(filelist)} Completed\")\n",
    "            try: \n",
    "                self.load_example(sourcefile,targetfile,dmodel)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\",e)\n",
    "                continue\n",
    "        imageio.mimsave('movie.gif', self.image_list)\n",
    "    def load_image(self,sourcepath,targetpath,dmodel):\n",
    "        \n",
    "        src_color_image_path         = os.path.join(datapath,\"color\",sourcepath)\n",
    "        src_depth_image_path         = os.path.join(datapath,\"depth\",sourcepath)\n",
    "        tgt_color_image_path         = os.path.join(datapath,\"color\",targetpath)\n",
    "        tgt_depth_image_path         = os.path.join(datapath,\"color\",sourcepath)\n",
    "\n",
    "        # Make sure to use the intrinsics corresponding to the seq_id above!!  \n",
    "        intrinsics = {\n",
    "            \"fx\": 525.0,\n",
    "            \"fy\": 525.0,\n",
    "            \"cx\": 319.5,\n",
    "            \"cy\": 239.5\n",
    "        }\n",
    "\n",
    "        # Some params for coloring the predicted correspondence confidences\n",
    "        weight_thr = 0.3\n",
    "        weight_scale = 1\n",
    "\n",
    "\n",
    "        image_height = opt.image_height\n",
    "        image_width  = opt.image_width\n",
    "        max_boundary_dist = opt.max_boundary_dist\n",
    "\n",
    "        \n",
    "        \n",
    "        # Source color and depth\n",
    "        source, _, cropper = dataset.DeformDataset.load_image(\n",
    "            src_color_image_path, src_depth_image_path, intrinsics, image_height, image_width\n",
    "        )\n",
    "\n",
    "        # Target color and depth (and boundary mask)\n",
    "        target, target_boundary_mask, _ = dataset.DeformDataset.load_image(\n",
    "            tgt_color_image_path, tgt_depth_image_path, intrinsics, image_height, image_width, cropper=cropper,\n",
    "            max_boundary_dist=max_boundary_dist, compute_boundary_mask=True\n",
    "        )\n",
    "\n",
    "        # Graph\n",
    "        graph_nodes = dmodel['graph_nodes']\n",
    "        graph_edges = dmodel['graph_edges']\n",
    "        graph_edges_weights = dmodel['graph_edges_weights']\n",
    "        graph_clusters = dmodel['graph_clusters']\n",
    "        pixel_anchors  = dmodel['pixel_anchors']\n",
    "        pixel_weights = dmodel['pixel_weights']\n",
    "        \n",
    "        num_nodes = np.array(graph_nodes.shape[0], dtype=np.int64)\n",
    "                \n",
    "        # Update intrinsics to reflect the crops\n",
    "        fx, fy, cx, cy = image_proc.modify_intrinsics_due_to_cropping(\n",
    "            intrinsics['fx'], intrinsics['fy'], intrinsics['cx'], intrinsics['cy'], \n",
    "            image_height, image_width, original_h=cropper.h, original_w=cropper.w\n",
    "        )\n",
    "\n",
    "        intrinsics = np.zeros((4), dtype=np.float32)\n",
    "        intrinsics[0] = fx\n",
    "        intrinsics[1] = fy\n",
    "        intrinsics[2] = cx\n",
    "        intrinsics[3] = cy\n",
    "        \n",
    "\n",
    "        #####################################################################################################\n",
    "        # Predict deformation\n",
    "        #####################################################################################################\n",
    "\n",
    "        # Move to device and unsqueeze in the batch dimension (to have batch size 1)\n",
    "        source_cuda               = torch.from_numpy(source).cuda().unsqueeze(0)\n",
    "        target_cuda               = torch.from_numpy(target).cuda().unsqueeze(0)\n",
    "        target_boundary_mask_cuda = torch.from_numpy(target_boundary_mask).cuda().unsqueeze(0)\n",
    "        graph_nodes_cuda          = torch.from_numpy(graph_nodes).cuda().unsqueeze(0)\n",
    "        graph_edges_cuda          = torch.from_numpy(graph_edges).cuda().unsqueeze(0)\n",
    "        graph_edges_weights_cuda  = torch.from_numpy(graph_edges_weights).cuda().unsqueeze(0)\n",
    "        graph_clusters_cuda       = torch.from_numpy(graph_clusters).cuda().unsqueeze(0)\n",
    "        pixel_anchors_cuda        = torch.from_numpy(pixel_anchors).cuda().unsqueeze(0)\n",
    "        pixel_weights_cuda        = torch.from_numpy(pixel_weights).cuda().unsqueeze(0)\n",
    "        intrinsics_cuda           = torch.from_numpy(intrinsics).cuda().unsqueeze(0)\n",
    "\n",
    "        num_nodes_cuda            = torch.from_numpy(num_nodes).cuda().unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_data = self.model(\n",
    "                source_cuda, target_cuda, \n",
    "                graph_nodes_cuda, graph_edges_cuda, graph_edges_weights_cuda, graph_clusters_cuda, \n",
    "                pixel_anchors_cuda, pixel_weights_cuda, \n",
    "                num_nodes_cuda, intrinsics_cuda, \n",
    "                evaluate=True, split=\"test\"\n",
    "            )\n",
    "\n",
    "        # Get some of the results\n",
    "        rotations_pred    = model_data[\"node_rotations\"].view(num_nodes, 3, 3).cpu().numpy()\n",
    "        translations_pred = model_data[\"node_translations\"].view(num_nodes, 3).cpu().numpy()\n",
    "\n",
    "        mask_pred = model_data[\"mask_pred\"]\n",
    "        assert mask_pred is not None, \"Make sure use_mask=True in options.py\"\n",
    "        mask_pred = mask_pred.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "\n",
    "        # Compute mask gt for mask baseline\n",
    "        _, source_points, valid_source_points, target_matches, \\\n",
    "            valid_target_matches, valid_correspondences, _, \\\n",
    "                _ = model_data[\"correspondence_info\"]\n",
    "\n",
    "        target_matches        = target_matches.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "        valid_source_points   = valid_source_points.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "        valid_target_matches  = valid_target_matches.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "        valid_correspondences = valid_correspondences.view(-1, opt.image_height, opt.image_width).cpu().numpy()\n",
    "\n",
    "        deformed_graph_nodes = graph_nodes + translations_pred        \n",
    "        \n",
    "        del source_cuda\n",
    "        del target_cuda\n",
    "        del target_boundary_mask_cuda\n",
    "        del graph_nodes_cuda\n",
    "        del graph_edges_cuda\n",
    "        del graph_edges_weights_cuda\n",
    "        del graph_clusters_cuda\n",
    "        del pixel_anchors_cuda\n",
    "        del pixel_weights_cuda\n",
    "        del intrinsics_cuda\n",
    "        \n",
    "        source_flat = np.moveaxis(source, 0, -1).reshape(-1, 6)\n",
    "        source_points = viz_utils.transform_pointcloud_to_opengl_coords(source_flat[..., 3:])\n",
    "        source_colors = source_flat[..., :3]\n",
    "\n",
    "        source_pcd = o3d.geometry.PointCloud()\n",
    "        source_pcd.points = o3d.utility.Vector3dVector(source_points)\n",
    "        source_pcd.colors = o3d.utility.Vector3dVector(source_colors)\n",
    "\n",
    "        # keep only object using the mask\n",
    "        valid_source_mask = np.moveaxis(valid_source_points, 0, -1).reshape(-1).astype(np.bool)\n",
    "        valid_source_points = source_points[valid_source_mask, :]\n",
    "        valid_source_colors = source_colors[valid_source_mask, :]\n",
    "        # source object PointCloud\n",
    "        source_object_pcd = o3d.geometry.PointCloud()\n",
    "        source_object_pcd.points = o3d.utility.Vector3dVector(valid_source_points)\n",
    "        source_object_pcd.colors = o3d.utility.Vector3dVector(valid_source_colors)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([source_pcd])\n",
    "        # o3d.visualization.draw_geometries([source_object_pcd])\n",
    "\n",
    "        #####################################################################################################\n",
    "        # Source warped\n",
    "        #####################################################################################################\n",
    "        warped_deform_pred_3d_np = image_proc.warp_deform_3d(\n",
    "            source, pixel_anchors, pixel_weights, graph_nodes, rotations_pred, translations_pred\n",
    "        )\n",
    "\n",
    "        source_warped = np.copy(source)\n",
    "        source_warped[3:, :, :] = warped_deform_pred_3d_np\n",
    "\n",
    "        # (source) warped RGB-D image\n",
    "        source_warped = np.moveaxis(source_warped, 0, -1).reshape(-1, 6)\n",
    "        warped_points = viz_utils.transform_pointcloud_to_opengl_coords(source_warped[..., 3:])\n",
    "        warped_colors = source_warped[..., :3]\n",
    "        # Filter points at (0, 0, 0)\n",
    "        warped_points = warped_points[valid_source_mask]\n",
    "        warped_colors = warped_colors[valid_source_mask]\n",
    "        # warped PointCloud\n",
    "        warped_pcd = o3d.geometry.PointCloud()\n",
    "        warped_pcd.points = o3d.utility.Vector3dVector(warped_points)\n",
    "        warped_pcd.paint_uniform_color([1, 0.706, 0]) # warped_pcd.colors = o3d.utility.Vector3dVector(warped_colors)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([source_object_pcd, warped_pcd])\n",
    "\n",
    "        ####################################\n",
    "        # TARGET #\n",
    "        ####################################\n",
    "        # target RGB-D image\n",
    "        target_flat = np.moveaxis(target, 0, -1).reshape(-1, 6)\n",
    "        target_points = viz_utils.transform_pointcloud_to_opengl_coords(target_flat[..., 3:])\n",
    "        target_colors = target_flat[..., :3]\n",
    "        # target PointCloud\n",
    "        target_pcd = o3d.geometry.PointCloud()\n",
    "        target_pcd.points = o3d.utility.Vector3dVector(target_points)\n",
    "        target_pcd.colors = o3d.utility.Vector3dVector(target_colors)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([target_pcd])\n",
    "\n",
    "        ####################################\n",
    "        # GRAPH #\n",
    "        ####################################\n",
    "\n",
    "        # Transform to OpenGL coords\n",
    "        graph_nodes = viz_utils.transform_pointcloud_to_opengl_coords(graph_nodes)\n",
    "        deformed_graph_nodes = viz_utils.transform_pointcloud_to_opengl_coords(deformed_graph_nodes)\n",
    "\n",
    "        # Graph nodes\n",
    "        rendered_graph_nodes = []\n",
    "        for node in graph_nodes:\n",
    "            mesh_sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.01)\n",
    "            mesh_sphere.compute_vertex_normals()\n",
    "            mesh_sphere.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "            mesh_sphere.translate(node)\n",
    "            rendered_graph_nodes.append(mesh_sphere)\n",
    "\n",
    "        # Merge all different sphere meshes\n",
    "        rendered_graph_nodes = viz_utils.merge_meshes(rendered_graph_nodes)\n",
    "\n",
    "        # Graph edges\n",
    "        edges_pairs = []\n",
    "        for node_id, edges in enumerate(graph_edges):\n",
    "            for neighbor_id in edges:\n",
    "                if neighbor_id == -1:\n",
    "                    break\n",
    "                edges_pairs.append([node_id, neighbor_id])    \n",
    "\n",
    "        colors = [[0.2, 1.0, 0.2] for i in range(len(edges_pairs))]\n",
    "        line_mesh = line_mesh_utils.LineMesh(graph_nodes, edges_pairs, colors, radius=0.003)\n",
    "        line_mesh_geoms = line_mesh.cylinder_segments\n",
    "\n",
    "        # Merge all different line meshes\n",
    "        line_mesh_geoms = viz_utils.merge_meshes(line_mesh_geoms)\n",
    "\n",
    "        # o3d.visualization.draw_geometries([rendered_graph_nodes, line_mesh_geoms, source_object_pcd])\n",
    "\n",
    "        # Combined nodes & edges\n",
    "        rendered_graph = [rendered_graph_nodes, line_mesh_geoms]\n",
    "\n",
    "        ####################################\n",
    "        # Mask\n",
    "        ####################################\n",
    "        mask_pred_flat = mask_pred.reshape(-1)\n",
    "        valid_correspondences = valid_correspondences.reshape(-1).astype(np.bool)\n",
    "\n",
    "        ####################################\n",
    "        # Correspondences\n",
    "        ####################################\n",
    "        # target matches\n",
    "        target_matches = np.moveaxis(target_matches, 0, -1).reshape(-1, 3)\n",
    "        target_matches = viz_utils.transform_pointcloud_to_opengl_coords(target_matches)\n",
    "\n",
    "        ################################\n",
    "        # \"Good\" matches\n",
    "        ################################\n",
    "        good_mask = valid_correspondences & (mask_pred_flat >= weight_thr)\n",
    "        good_source_points_corresp  = source_points[good_mask]\n",
    "        good_target_matches_corresp = target_matches[good_mask]\n",
    "        good_mask_pred              = mask_pred_flat[good_mask]\n",
    "\n",
    "        # number of good matches\n",
    "        n_good_matches = good_source_points_corresp.shape[0]\n",
    "        # Subsample\n",
    "        subsample = True\n",
    "        if subsample:\n",
    "            N = 2000\n",
    "            sampled_idxs = np.random.permutation(n_good_matches)[:N]\n",
    "            good_source_points_corresp  = good_source_points_corresp[sampled_idxs]\n",
    "            good_target_matches_corresp = good_target_matches_corresp[sampled_idxs]\n",
    "            good_mask_pred              = good_mask_pred[sampled_idxs]\n",
    "            n_good_matches = N\n",
    "        # both good_source and good_target points together into one vector\n",
    "        good_matches_points = np.concatenate([good_source_points_corresp, good_target_matches_corresp], axis=0)\n",
    "        good_matches_lines = [[i, i + n_good_matches] for i in range(0, n_good_matches, 1)]\n",
    "\n",
    "        # --> Create good (unweighted) lines \n",
    "        good_matches_colors = [[201/255, 177/255, 14/255] for i in range(len(good_matches_lines))]\n",
    "        good_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(good_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(good_matches_lines),\n",
    "        )\n",
    "        good_matches_set.colors = o3d.utility.Vector3dVector(good_matches_colors)\n",
    "\n",
    "        # --> Create good weighted lines \n",
    "        # first, we need to get the proper color coding\n",
    "        high_color, low_color = np.array([0.0, 0.8, 0]), np.array([0.8, 0, 0.0])\n",
    "\n",
    "        good_weighted_matches_colors = np.ones_like(good_source_points_corresp)\n",
    "\n",
    "        weights_normalized = np.maximum(np.minimum(0.5 + (good_mask_pred - weight_thr) / weight_scale, 1.0), 0.0)\n",
    "        weights_normalized_opposite = 1 - weights_normalized\n",
    "\n",
    "        good_weighted_matches_colors[:, 0] = weights_normalized * high_color[0] + weights_normalized_opposite * low_color[0]\n",
    "        good_weighted_matches_colors[:, 1] = weights_normalized * high_color[1] + weights_normalized_opposite * low_color[1]\n",
    "        good_weighted_matches_colors[:, 2] = weights_normalized * high_color[2] + weights_normalized_opposite * low_color[2]\n",
    "\n",
    "        good_weighted_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(good_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(good_matches_lines),\n",
    "        )\n",
    "        good_weighted_matches_set.colors = o3d.utility.Vector3dVector(good_weighted_matches_colors)\n",
    "\n",
    "\n",
    "        ################################\n",
    "        # \"Bad\" matches\n",
    "        ################################\n",
    "        bad_mask = valid_correspondences & (mask_pred_flat < weight_thr)\n",
    "        bad_source_points_corresp  = source_points[bad_mask]\n",
    "        bad_target_matches_corresp = target_matches[bad_mask]\n",
    "        bad_mask_pred              = mask_pred_flat[bad_mask]\n",
    "\n",
    "        # number of good matches\n",
    "        n_bad_matches = bad_source_points_corresp.shape[0]\n",
    "\n",
    "        # both good_source and good_target points together into one vector\n",
    "        bad_matches_points = np.concatenate([bad_source_points_corresp, bad_target_matches_corresp], axis=0)\n",
    "        bad_matches_lines = [[i, i + n_bad_matches] for i in range(0, n_bad_matches, 1)]\n",
    "\n",
    "        # --> Create bad (unweighted) lines \n",
    "        bad_matches_colors = [[201/255, 177/255, 14/255] for i in range(len(bad_matches_lines))]\n",
    "        bad_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(bad_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(bad_matches_lines),\n",
    "        )\n",
    "        bad_matches_set.colors = o3d.utility.Vector3dVector(bad_matches_colors)\n",
    "\n",
    "        # --> Create bad weighted lines \n",
    "        # first, we need to get the proper color coding\n",
    "        high_color, low_color = np.array([0.0, 0.8, 0]), np.array([0.8, 0, 0.0])\n",
    "\n",
    "        bad_weighted_matches_colors = np.ones_like(bad_source_points_corresp)\n",
    "\n",
    "        weights_normalized = np.maximum(np.minimum(0.5 + (bad_mask_pred - weight_thr) / weight_scale, 1.0), 0.0)\n",
    "        weights_normalized_opposite = 1 - weights_normalized\n",
    "\n",
    "        bad_weighted_matches_colors[:, 0] = weights_normalized * high_color[0] + weights_normalized_opposite * low_color[0]\n",
    "        bad_weighted_matches_colors[:, 1] = weights_normalized * high_color[1] + weights_normalized_opposite * low_color[1]\n",
    "        bad_weighted_matches_colors[:, 2] = weights_normalized * high_color[2] + weights_normalized_opposite * low_color[2]\n",
    "\n",
    "        bad_weighted_matches_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(bad_matches_points),\n",
    "            lines=o3d.utility.Vector2iVector(bad_matches_lines),\n",
    "        )\n",
    "        bad_weighted_matches_set.colors = o3d.utility.Vector3dVector(bad_weighted_matches_colors)\n",
    "\n",
    "        ####################################\n",
    "        # Generate info for aligning source to target (by interpolating between source and warped source)\n",
    "        ####################################\n",
    "        assert warped_points.shape[0] == valid_source_points.shape[0]\n",
    "        line_segments = warped_points - valid_source_points\n",
    "        line_segments_unit, line_lengths = line_mesh_utils.normalized(line_segments)\n",
    "        line_lengths = line_lengths[:, np.newaxis]\n",
    "        line_lengths = np.repeat(line_lengths, 3, axis=1)\n",
    "\n",
    "        ####################################\n",
    "        # Draw \n",
    "        ####################################\n",
    "        geometry_dict = {\n",
    "            \"source_pcd\": source_pcd, \n",
    "            \"source_obj\": source_object_pcd, \n",
    "            \"target_pcd\": target_pcd, \n",
    "            \"graph\":      rendered_graph,\n",
    "            # \"deformed_graph\":    rendered_deformed_graph\n",
    "        }\n",
    "\n",
    "        alignment_dict = {\n",
    "            \"valid_source_points\": valid_source_points,\n",
    "            \"line_segments_unit\":  line_segments_unit,\n",
    "            \"line_lengths\":        line_lengths\n",
    "        }\n",
    "\n",
    "        matches_dict = {\n",
    "            \"good_matches_set\":          good_matches_set,\n",
    "            \"good_weighted_matches_set\": good_weighted_matches_set,\n",
    "            \"bad_matches_set\":           bad_matches_set,\n",
    "            \"bad_weighted_matches_set\":  bad_weighted_matches_set\n",
    "        }\n",
    "\n",
    "        # Save image \n",
    "        sourceimage = create_image([source_object_pcd,good_weighted_matches_set])\n",
    "        warpedimage = create_image([warped_pcd])\n",
    "        targetimage = create_image([target_pcd],w=1280,h=960)\n",
    "        image = cv2.hconcat([cv2.vconcat([sourceimage,warpedimage]),targetimage])\n",
    "        \n",
    "        self.image_list.append(image)\n",
    "        #####################################################################################################\n",
    "        # Open viewer\n",
    "        #####################################################################################################\n",
    "#         manager = viz_utils.CustomDrawGeometryWithKeyCallback(\n",
    "#             geometry_dict, alignment_dict, matches_dict\n",
    "#         )\n",
    "#         manager.custom_draw_geometry_with_key_callback()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Model: Initialization Complete, Node:89 Edges:44\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8a762c8d7e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRGBDVideoResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../DeepDeform/new_test/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgridsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-95f437563c71>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datapath, source_id, gridsize)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeformableModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"depth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msourcefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgridsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtargetfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{i}/{len(filelist)} Completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msourcefile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "RGBDVideoResults(datapath=\"../DeepDeform/new_test/\",gridsize=0.0006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"../DeepDeform/new_test/\"\n",
    "sorted(os.listdir(datapath),key=lambda x: int(x.split('.')[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
